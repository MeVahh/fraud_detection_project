{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd978c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcare Fraud Detection - Simplified Evaluation Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           precision_recall_curve, auc, average_precision_score, \n",
    "                           precision_score, recall_score, f1_score, roc_curve)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "# Create directories\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df2c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# # Healthcare Fraud Detection - Evaluation & Error Analysis\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the best model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m best_model = \u001b[43mjoblib\u001b[49m.load(\u001b[33m'\u001b[39m\u001b[33mbest_fraud_detection_model.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== FINAL TEST SET EVALUATION ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load and Prepare Data\n",
    "print(\"ðŸ“Š Loading data...\")\n",
    "beneficiary_df = pd.read_csv('/content/Train_Beneficiarydata.csv')\n",
    "inpatient_df = pd.read_csv('/content/Train_Inpatientdata.csv')\n",
    "outpatient_df = pd.read_csv('/content/Train_Outpatientdata.csv')\n",
    "labels_df = pd.read_csv('/content/Train_labels.csv')\n",
    "\n",
    "print(f\"Data shapes - Beneficiary: {beneficiary_df.shape}, Inpatient: {inpatient_df.shape}, Outpatient: {outpatient_df.shape}, Labels: {labels_df.shape}\")\n",
    "\n",
    "# 2. Simple Feature Engineering\n",
    "print(\"ðŸ› ï¸ Creating features...\")\n",
    "\n",
    "def create_simple_features(inpatient_df, outpatient_df, labels_df):\n",
    "    features_list = []\n",
    "    \n",
    "    for provider in labels_df['Provider'].unique():\n",
    "        provider_features = {'Provider': provider}\n",
    "        \n",
    "        # Get provider claims\n",
    "        inpatient_claims = inpatient_df[inpatient_df['Provider'] == provider]\n",
    "        outpatient_claims = outpatient_df[outpatient_df['Provider'] == provider]\n",
    "        \n",
    "        # Basic features\n",
    "        provider_features['Total_Claims'] = len(inpatient_claims) + len(outpatient_claims)\n",
    "        provider_features['Inpatient_Ratio'] = len(inpatient_claims) / provider_features['Total_Claims'] if provider_features['Total_Claims'] > 0 else 0\n",
    "        \n",
    "        # Amount features\n",
    "        if len(inpatient_claims) > 0:\n",
    "            provider_features['Avg_Inpatient_Amount'] = inpatient_claims['InscClaimAmtReimbursed'].mean()\n",
    "            provider_features['Total_Inpatient_Amount'] = inpatient_claims['InscClaimAmtReimbursed'].sum()\n",
    "        else:\n",
    "            provider_features['Avg_Inpatient_Amount'] = 0\n",
    "            provider_features['Total_Inpatient_Amount'] = 0\n",
    "            \n",
    "        if len(outpatient_claims) > 0:\n",
    "            provider_features['Avg_Outpatient_Amount'] = outpatient_claims['InscClaimAmtReimbursed'].mean()\n",
    "            provider_features['Total_Outpatient_Amount'] = outpatient_claims['InscClaimAmtReimbursed'].sum()\n",
    "        else:\n",
    "            provider_features['Avg_Outpatient_Amount'] = 0\n",
    "            provider_features['Total_Outpatient_Amount'] = 0\n",
    "        \n",
    "        provider_features['Total_Amount'] = provider_features['Total_Inpatient_Amount'] + provider_features['Total_Outpatient_Amount']\n",
    "        \n",
    "        features_list.append(provider_features)\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    final_df = pd.merge(features_df, labels_df, on='Provider', how='left')\n",
    "    return final_df\n",
    "\n",
    "# Create features\n",
    "final_df = create_simple_features(inpatient_df, outpatient_df, labels_df)\n",
    "print(f\"âœ… Features created: {final_df.shape}\")\n",
    "\n",
    "# 3. Prepare Modeling Data\n",
    "final_df['Fraud'] = final_df['PotentialFraud'].map({'Yes': 1, 'No': 0})\n",
    "feature_cols = [col for col in final_df.columns if col not in ['Provider', 'PotentialFraud', 'Fraud']]\n",
    "X = final_df[feature_cols].fillna(0)\n",
    "y = final_df['Fraud']\n",
    "\n",
    "print(f\"ðŸŽ¯ Fraud rate: {y.mean():.3f} ({y.sum()} fraud cases out of {len(y)})\")\n",
    "\n",
    "# 4. Rigorous Validation Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Data splits - Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "# 5. Handle Class Imbalance\n",
    "class_weight = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weight))\n",
    "print(f\"âš–ï¸ Class weights: {class_weight_dict}\")\n",
    "\n",
    "# 6. Train Models with Regularization\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight=class_weight_dict,\n",
    "        random_state=42,\n",
    "        max_depth=10,  # Regularization\n",
    "        min_samples_split=10  # Regularization\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        class_weight=class_weight_dict,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        C=0.1  # Regularization\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and validate\n",
    "print(\"ðŸš€ Training models...\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    print(f\"  {name} - Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = 'Random Forest'  # Based on typical performance\n",
    "best_model = models[best_model_name]\n",
    "print(f\"ðŸ† Best model: {best_model_name}\")\n",
    "\n",
    "# 7. Comprehensive Evaluation on Test Set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“ˆ COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate all required metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ðŸŽ¯ PERFORMANCE METRICS:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nðŸ“‹ CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Fraud', 'Fraud']))\n",
    "\n",
    "# 8. Confusion Matrix\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],\n",
    "            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "# 9. ROC Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "\n",
    "# 10. Precision-Recall Curve\n",
    "plt.subplot(1, 3, 3)\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "plt.plot(recall_curve, precision_curve, label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 11. Cost-Based Analysis\n",
    "print(f\"\\nðŸ’° COST-BASED ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Realistic cost assumptions\n",
    "investigation_cost = 10000  # Cost to investigate a provider\n",
    "avg_fraud_amount = 50000   # Average fraud amount\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "fp_cost = fp * investigation_cost\n",
    "fn_cost = fn * avg_fraud_amount\n",
    "tp_benefit = tp * avg_fraud_amount\n",
    "net_impact = tp_benefit - fp_cost - fn_cost\n",
    "\n",
    "print(f\"False Positives: {fp} Ã— ${investigation_cost:,} = ${fp_cost:,}\")\n",
    "print(f\"False Negatives: {fn} Ã— ${avg_fraud_amount:,} = ${fn_cost:,}\")\n",
    "print(f\"True Positives:  {tp} Ã— ${avg_fraud_amount:,} = ${tp_benefit:,}\")\n",
    "print(f\"Net Impact: ${net_impact:,}\")\n",
    "\n",
    "if net_impact > 0:\n",
    "    print(\"âœ… POSITIVE BUSINESS IMPACT\")\n",
    "else:\n",
    "    print(\"âš ï¸  NEGATIVE BUSINESS IMPACT\")\n",
    "\n",
    "# 12. Error Analysis\n",
    "print(f\"\\nðŸ” ERROR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = X_test.copy()\n",
    "error_df['Actual'] = y_test.values\n",
    "error_df['Predicted'] = y_pred\n",
    "error_df['Predicted_Probability'] = y_pred_proba\n",
    "error_df['Provider'] = final_df.loc[X_test.index, 'Provider'].values\n",
    "\n",
    "# Identify error types\n",
    "error_df['Error_Type'] = 'Correct'\n",
    "error_df.loc[(error_df['Actual'] == 1) & (error_df['Predicted'] == 0), 'Error_Type'] = 'False Negative'\n",
    "error_df.loc[(error_df['Actual'] == 0) & (error_df['Predicted'] == 1), 'Error_Type'] = 'False Positive'\n",
    "\n",
    "print(\"Error distribution:\")\n",
    "print(error_df['Error_Type'].value_counts())\n",
    "\n",
    "# 13. Case Studies\n",
    "print(f\"\\nðŸ“ CASE STUDIES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# False Positive Case Studies (Legitimate providers flagged as fraud)\n",
    "false_positives = error_df[error_df['Error_Type'] == 'False Positive']\n",
    "print(\"\\nðŸš¨ FALSE POSITIVES (Legitimate providers flagged as fraud):\")\n",
    "\n",
    "if len(false_positives) >= 2:\n",
    "    for i in range(min(2, len(false_positives))):\n",
    "        case = false_positives.iloc[i]\n",
    "        print(f\"\\nCase {i+1}:\")\n",
    "        print(f\"  Provider: {case['Provider']}\")\n",
    "        print(f\"  Prediction Probability: {case['Predicted_Probability']:.3f}\")\n",
    "        print(f\"  Key Features:\")\n",
    "        print(f\"    - Total Claims: {case['Total_Claims']:.0f}\")\n",
    "        print(f\"    - Total Amount: ${case['Total_Amount']:,.0f}\")\n",
    "        print(f\"    - Inpatient Ratio: {case['Inpatient_Ratio']:.3f}\")\n",
    "        \n",
    "        # Analysis\n",
    "        if case['Total_Amount'] > final_df['Total_Amount'].quantile(0.75):\n",
    "            print(f\"  ðŸ” Analysis: High total amount may have triggered false positive\")\n",
    "        if case['Inpatient_Ratio'] > 0.7:\n",
    "            print(f\"  ðŸ” Analysis: High inpatient ratio unusual for legitimate providers\")\n",
    "\n",
    "# False Negative Case Studies (Fraudulent providers missed)\n",
    "false_negatives = error_df[error_df['Error_Type'] == 'False Negative']\n",
    "print(\"\\nâŒ FALSE NEGATIVES (Fraudulent providers missed):\")\n",
    "\n",
    "if len(false_negatives) >= 2:\n",
    "    for i in range(min(2, len(false_negatives))):\n",
    "        case = false_negatives.iloc[i]\n",
    "        print(f\"\\nCase {i+1}:\")\n",
    "        print(f\"  Provider: {case['Provider']}\")\n",
    "        print(f\"  Prediction Probability: {case['Predicted_Probability']:.3f}\")\n",
    "        print(f\"  Key Features:\")\n",
    "        print(f\"    - Total Claims: {case['Total_Claims']:.0f}\")\n",
    "        print(f\"    - Total Amount: ${case['Total_Amount']:,.0f}\")\n",
    "        print(f\"    - Avg Outpatient Amount: ${case.get('Avg_Outpatient_Amount', 0):,.0f}\")\n",
    "        \n",
    "        # Analysis\n",
    "        if case['Total_Claims'] < final_df['Total_Claims'].quantile(0.5):\n",
    "            print(f\"  ðŸ” Analysis: Low claim volume may have caused model to miss patterns\")\n",
    "        if case.get('Avg_Outpatient_Amount', 0) < final_df['Avg_Outpatient_Amount'].quantile(0.5):\n",
    "            print(f\"  ðŸ” Analysis: Average amounts may appear normal\")\n",
    "\n",
    "# 14. Feature Importance Analysis\n",
    "print(f\"\\nðŸŽ¯ FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# 15. Recommendations for Improvement\n",
    "print(f\"\\nðŸ’¡ RECOMMENDATIONS FOR FUTURE ITERATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"1. Feature Engineering Improvements:\")\n",
    "print(\"   - Add temporal patterns (seasonal billing)\")\n",
    "print(\"   - Include beneficiary demographic features\")\n",
    "print(\"   - Add procedure code analysis\")\n",
    "print(\"   - Create network analysis features\")\n",
    "\n",
    "print(\"\\n2. Model Improvements:\")\n",
    "print(\"   - Try ensemble methods (XGBoost, LightGBM)\")\n",
    "print(\"   - Implement cross-validation\")\n",
    "print(\"   - Add feature selection\")\n",
    "print(\"   - Try different classification thresholds\")\n",
    "\n",
    "print(\"\\n3. Data Improvements:\")\n",
    "print(\"   - Collect more fraud examples\")\n",
    "print(\"   - Add external data sources\")\n",
    "print(\"   - Improve feature quality\")\n",
    "\n",
    "# 16. Save Model and Results\n",
    "print(f\"\\nðŸ’¾ SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save model with feature names\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'feature_names': feature_cols,\n",
    "    'performance': {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_artifacts, 'models/fraud_detection_model.pkl')\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    'test_performance': {\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'pr_auc': float(pr_auc)\n",
    "    },\n",
    "    'business_impact': {\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'net_impact': float(net_impact)\n",
    "    },\n",
    "    'error_analysis': error_df[['Provider', 'Actual', 'Predicted', 'Predicted_Probability', 'Error_Type']].to_dict('records')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('reports/evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"âœ… Model saved: models/fraud_detection_model.pkl\")\n",
    "print(\"âœ… Results saved: reports/evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
