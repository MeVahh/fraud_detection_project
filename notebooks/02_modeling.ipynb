{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6202080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Healthcare Fraud Detection - Modeling\n",
    "# \n",
    "# ## 2.1 Data Preparation for Modeling\n",
    "\n",
    "# %%\n",
    "# Load the preprocessed data\n",
    "# Assuming final_df is available from previous notebook\n",
    "def prepare_modeling_data(df):\n",
    "    \"\"\"Prepare data for modeling\"\"\"\n",
    "    if df.empty:\n",
    "        # Create sample data for demonstration\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        df = pd.DataFrame({\n",
    "            'Provider': [f'PRV{i:05d}' for i in range(n_samples)],\n",
    "            'Total_Claims': np.random.poisson(100, n_samples),\n",
    "            'Total_Amount_Reimbursed': np.random.exponential(50000, n_samples),\n",
    "            'Avg_Inpatient_Claim_Amount': np.random.normal(1000, 300, n_samples),\n",
    "            'Inpatient_Claim_Ratio': np.random.beta(2, 5, n_samples),\n",
    "            'Unique_AttendingPhysicians_Inpatient': np.random.poisson(5, n_samples),\n",
    "            'PotentialFraud': np.random.choice(['Yes', 'No'], n_samples, p=[0.1, 0.9])\n",
    "        })\n",
    "\n",
    "    # Encode target variable\n",
    "    df['Fraud'] = df['PotentialFraud'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "    # Select features for modeling\n",
    "    feature_columns = [col for col in df.columns if col not in ['Provider', 'PotentialFraud', 'Fraud']]\n",
    "    X = df[feature_columns]\n",
    "    y = df['Fraud']\n",
    "\n",
    "    return X, y, df\n",
    "\n",
    "# %%\n",
    "X, y, modeling_df = prepare_modeling_data(final_df)\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Fraud rate: {y.mean():.3f}\")\n",
    "\n",
    "# %%\n",
    "# Handle missing values\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "\n",
    "# ## 2.2 Class Imbalance Handling\n",
    "\n",
    "# %%\n",
    "def handle_class_imbalance(X_train, y_train, method='class_weight'):\n",
    "    \"\"\"Handle class imbalance using different strategies\"\"\"\n",
    "    if method == 'class_weight':\n",
    "        # Calculate class weights\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        class_weight = dict(zip(classes, weights))\n",
    "        return class_weight, X_train, y_train\n",
    "\n",
    "    elif method == 'smote':\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        return None, X_resampled, y_resampled\n",
    "\n",
    "    elif method == 'undersampling':\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "        return None, X_resampled, y_resampled\n",
    "\n",
    "# %%\n",
    "# Apply class imbalance handling\n",
    "class_weight, X_train_balanced, y_train_balanced = handle_class_imbalance(X_train, y_train, method='class_weight')\n",
    "\n",
    "print(\"Original training distribution:\", np.bincount(y_train))\n",
    "print(\"Balanced training distribution:\", np.bincount(y_train_balanced))\n",
    "\n",
    "# ## 2.3 Model Training\n",
    "\n",
    "# %%\n",
    "def train_models(X_train, y_train, class_weight=None):\n",
    "    \"\"\"Train multiple models for comparison\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # 1. Logistic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight=class_weight,\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    lr.fit(X_train, y_train)\n",
    "    models['Logistic Regression'] = lr\n",
    "\n",
    "    # 2. Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight=class_weight,\n",
    "        random_state=42,\n",
    "        max_depth=10\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    models['Random Forest'] = rf\n",
    "\n",
    "    # 3. Gradient Boosting\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        max_depth=6\n",
    "    )\n",
    "    gb.fit(X_train, y_train)\n",
    "    models['Gradient Boosting'] = gb\n",
    "\n",
    "    return models\n",
    "\n",
    "# %%\n",
    "# Train all models\n",
    "models = train_models(X_train_balanced, y_train_balanced, class_weight)\n",
    "\n",
    "print(\"Models trained successfully:\")\n",
    "for name, model in models.items():\n",
    "    print(f\"- {name}\")\n",
    "\n",
    "\n",
    "# ## 2.4 Model Validation\n",
    "\n",
    "# %%\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    roc_auc = roc_auc_score(y, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y, y_pred_proba)\n",
    "\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# %%\n",
    "# Evaluate on validation set\n",
    "print(\"=== VALIDATION SET PERFORMANCE ===\")\n",
    "validation_results = {}\n",
    "for name, model in models.items():\n",
    "    validation_results[name] = evaluate_model(model, X_val, y_val, name)\n",
    "\n",
    "\n",
    "# ## 2.5 Model Comparison\n",
    "\n",
    "# %%\n",
    "def compare_models(results_dict):\n",
    "    \"\"\"Compare model performance\"\"\"\n",
    "    comparison_df = pd.DataFrame(results_dict).T\n",
    "    comparison_df = comparison_df[['precision', 'recall', 'f1', 'roc_auc', 'pr_auc']]\n",
    "    return comparison_df\n",
    "\n",
    "# %%\n",
    "# Model comparison\n",
    "model_comparison = compare_models(validation_results)\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Visual comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics = ['precision', 'recall', 'f1', 'roc_auc', 'pr_auc']\n",
    "model_comparison[metrics].plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Select best model based on F1-score\n",
    "best_model_name = model_comparison['f1'].idxmax()\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'best_fraud_detection_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
